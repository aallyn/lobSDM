{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data libraries and load the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       activity_id       institution_id      source_id       experiment_id  \\\n",
      "0       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "1       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "2       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "3       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "4       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "...            ...                  ...            ...                 ...   \n",
      "514813        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514814        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514815        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514816        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514817        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "\n",
      "       member_id table_id variable_id grid_label  \\\n",
      "0       r1i1p1f1     Amon          ps         gn   \n",
      "1       r1i1p1f1     Amon        rsds         gn   \n",
      "2       r1i1p1f1     Amon        rlus         gn   \n",
      "3       r1i1p1f1     Amon        rlds         gn   \n",
      "4       r1i1p1f1     Amon         psl         gn   \n",
      "...          ...      ...         ...        ...   \n",
      "514813  r1i1p1f1     Amon         tas         gr   \n",
      "514814  r1i1p1f1     Amon        tauu         gr   \n",
      "514815  r1i1p1f1     Amon         hur         gr   \n",
      "514816  r1i1p1f1     Amon         hus         gr   \n",
      "514817  r1i1p1f1     Amon        tauv         gr   \n",
      "\n",
      "                                                   zstore  dcpp_init_year  \\\n",
      "0       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "1       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "2       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "3       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "4       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "...                                                   ...             ...   \n",
      "514813  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514814  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514815  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514816  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514817  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "\n",
      "         version  \n",
      "0       20170706  \n",
      "1       20170706  \n",
      "2       20170706  \n",
      "3       20170706  \n",
      "4       20170706  \n",
      "...          ...  \n",
      "514813  20211207  \n",
      "514814  20211207  \n",
      "514815  20211207  \n",
      "514816  20211207  \n",
      "514817  20211207  \n",
      "\n",
      "[514818 rows x 11 columns]\n",
      "Index(['activity_id', 'institution_id', 'source_id', 'experiment_id',\n",
      "       'member_id', 'table_id', 'variable_id', 'grid_label', 'zstore',\n",
      "       'dcpp_init_year', 'version'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from packaging.version import Version\n",
    "import numpy as np\n",
    "\n",
    "# Load the CMIP6 catalog\n",
    "catalog_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(catalog_url)\n",
    "print(col.df)\n",
    "print(col.df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for historical runs of models we already have projeciton information for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of model names: ['MIROC6_r1i1p1f1_historical', 'NESM3_r1i1p1f1_historical', 'IPSL-CM6A-LR_r1i1p1f1_historical', 'CESM2_r1i1p1f1_historical', 'CMCC-CM2-SR5_r1i1p1f1_historical', 'GISS-E2-1-G_r1i1p3f1_historical', 'EC-Earth3-Veg_r1i1p1f1_historical', 'EC-Earth3-Veg-LR_r1i1p1f1_historical', 'MIROC-ES2L_r1i1p1f2_historical', 'INM-CM4-8_r1i1p1f1_historical', 'FGOALS-f3-L_r1i1p1f1_historical', 'MRI-ESM2-0_r1i1p1f1_historical', 'GFDL-CM4_r1i1p1f1_historical', 'CanESM5_r1i1p2f1_historical', 'FGOALS-g3_r1i1p1f1_historical', 'NorESM2-LM_r1i1p1f1_historical', 'CNRM-CM6-1_r1i1p1f2_historical', 'EC-Earth3_r1i1p1f1_historical', 'HadGEM3-GC31-LL_r1i1p1f3_historical', 'CIESM_r1i1p1f1_historical', 'CanESM5-CanOE_r1i1p2f1_historical', 'UKESM1-0-LL_r1i1p1f2_historical', 'FIO-ESM-2-0_r1i1p1f1_historical', 'CESM2-WACCM_r1i1p1f1_historical', 'NorESM2-MM_r1i1p1f1_historical', 'CNRM-ESM2-1_r4i1p1f2_historical', 'INM-CM5-0_r1i1p1f1_historical']\n",
      "['CMCC-CM2-HR4' 'EC-Earth3P-HR' 'HadGEM3-GC31-MM' 'HadGEM3-GC31-HM'\n",
      " 'HadGEM3-GC31-LM' 'EC-Earth3P' 'ECMWF-IFS-HR' 'ECMWF-IFS-LR'\n",
      " 'HadGEM3-GC31-LL' 'CMCC-CM2-VHR4' 'GFDL-CM4' 'GFDL-AM4' 'IPSL-CM6A-LR'\n",
      " 'E3SM-1-0' 'CNRM-CM6-1' 'GFDL-ESM4' 'GFDL-ESM2M' 'GFDL-CM4C192'\n",
      " 'GFDL-OM4p5B' 'GISS-E2-1-G' 'GISS-E2-1-H' 'CNRM-ESM2-1' 'BCC-CSM2-MR'\n",
      " 'BCC-ESM1' 'MIROC6' 'AWI-CM-1-1-MR' 'EC-Earth3-LR' 'IPSL-CM6A-ATM-HR'\n",
      " 'CESM2' 'CESM2-WACCM' 'CNRM-CM6-1-HR' 'MRI-ESM2-0' 'SAM0-UNICON'\n",
      " 'GISS-E2-1-G-CC' 'UKESM1-0-LL' 'EC-Earth3' 'EC-Earth3-Veg' 'FGOALS-f3-L'\n",
      " 'CanESM5' 'CanESM5-CanOE' 'INM-CM4-8' 'INM-CM5-0' 'NESM3'\n",
      " 'MPI-ESM-1-2-HAM' 'CAMS-CSM1-0' 'MPI-ESM1-2-LR' 'MPI-ESM1-2-HR'\n",
      " 'MRI-AGCM3-2-H' 'MRI-AGCM3-2-S' 'MCM-UA-1-0' 'INM-CM5-H' 'KACE-1-0-G'\n",
      " 'NorESM2-LM' 'FGOALS-f3-H' 'FGOALS-g3' 'MIROC-ES2L' 'FIO-ESM-2-0'\n",
      " 'NorCPM1' 'NorESM1-F' 'MPI-ESM1-2-XR' 'CESM1-1-CAM5-CMIP5' 'E3SM-1-1'\n",
      " 'KIOST-ESM' 'ACCESS-CM2' 'NorESM2-MM' 'ACCESS-ESM1-5' 'IITM-ESM'\n",
      " 'GISS-E2-2-G' 'CESM2-FV2' 'GISS-E2-2-H' 'CESM2-WACCM-FV2' 'CIESM'\n",
      " 'E3SM-1-1-ECA' 'TaiESM1' 'AWI-ESM-1-1-LR' 'EC-Earth3-Veg-LR' 'CMCC-ESM2'\n",
      " 'CMCC-CM2-SR5' 'EC-Earth3-AerChem' 'IPSL-CM6A-LR-INCA' 'IPSL-CM5A2-INCA'\n",
      " 'BCC-CSM2-HR' 'EC-Earth3P-VHR' 'CESM1-WACCM-SC' 'CAS-ESM2-0'\n",
      " 'EC-Earth3-CC' 'MIROC-ES2H' 'ICON-ESM-LR']\n",
      "['r1i1p1f1', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p3f1', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p1f2', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p2f1', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p1f2', 'r1i1p1f1', 'r1i1p1f3', 'r1i1p1f1', 'r1i1p2f1', 'r1i1p1f2', 'r1i1p1f1', 'r1i1p1f1', 'r1i1p1f1', 'r4i1p1f2', 'r1i1p1f1']\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='35' class='' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [35/35 02:28&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the folder containing your existing CMIP6 models\n",
    "model_folder_path = \"/Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP5_85/BiasCorrected/IndividualModels/surf_temp\"  # replace with your folder path\n",
    "\n",
    "# Get list of model names from folder\n",
    "strings_to_remove = [\"surf_temp_stGrid_tos_\", \".grd\"]  # replace with actual strings to remove\n",
    "\n",
    "ssp585_models = []\n",
    "for name in os.listdir(model_folder_path):\n",
    "    # Only include specific file types if needed (e.g., .nc files)\n",
    "    if name.endswith(\"historical.grd\"):  # Optional: Filter by file extension\n",
    "        # Remove each unwanted string from the file name\n",
    "        clean_name = name\n",
    "        for string in strings_to_remove:\n",
    "            clean_name = clean_name.replace(string, \"\")\n",
    "        ssp585_models.append(clean_name)\n",
    "print(\"List of model names:\", ssp585_models)\n",
    "print(col.df['source_id'].unique())\n",
    "\n",
    "# Filter for historical data of the specified models\n",
    "# Extract the first and second parts\n",
    "# Split each model name by '_'\n",
    "split_names = [name.split('_') for name in ssp585_models]\n",
    "\n",
    "# Extract just the IDs (the last element in the split parts)\n",
    "source_ids = [parts[0] for parts in split_names]\n",
    "member_ids = [parts[1] for parts in split_names]\n",
    "print(member_ids)\n",
    "variable_ids = [\"thetao\", \"tos\"]\n",
    "table_ids = [\"Omon\"]\n",
    "\n",
    "historical_data = col.search(\n",
    "    experiment_id = 'historical',\n",
    "    source_id = source_ids,\n",
    "    member_id = member_ids,\n",
    "    variable_id = variable_ids,\n",
    "    table_id = table_ids)# Filter for the specific models and variants\n",
    "\n",
    "\n",
    "# historical_data_latest = historical_data.loc[historical_data.groupby(\n",
    "#     [\"source_id\", \"experiment_id\", \"variable_id\", \"table_id\", \"grid_label\"]\n",
    "# )[\"version\"].idxmax()]\n",
    "\n",
    "dset_dict = historical_data.to_dataset_dict(\n",
    "    xarray_open_kwargs={\"consolidated\": True, \"decode_times\": True, \"use_cftime\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and save the historical data for region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(20.54499053955078, 82.37899780273438, None)\n",
      "slice(254.0767822265625, 323.5, None)\n"
     ]
    }
   ],
   "source": [
    "# Define your region of interest (extent)\n",
    "# Load the NetCDF file using xarray\n",
    "ds = xr.open_dataset(\"/Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP1_26/RawTmpFiles/so_CanESM5_r10i1p1f1_ssp126.nc\")\n",
    "\n",
    "# Get the bounds (assuming the latitudes and longitudes are called 'lat' and 'lon')\n",
    "lat_bounds = slice(ds['latitude'].values.min(), ds['latitude'].values.max())\n",
    "print(lat_bounds)\n",
    "lon_bounds = slice(ds['longitude'].values.min(), ds['longitude'].values.max())\n",
    "print(lon_bounds)\n",
    "\n",
    "lat_bounds = (20, 83)\n",
    "lon_bounds = (254, 324)\n",
    "\n",
    "# Download\n",
    "box_root = \"/Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP5_85/RawTmpFiles/\"\n",
    " \n",
    "# # I couldn't get this to work!       \n",
    "# for key, ds in dset_dict.items():\n",
    "#     # print(f\"Model: {key}, Type of dataset: {type(ds)}\")\n",
    "#     file_name = f\"{box_root}{key}.nc\"\n",
    "#     try:\n",
    "#         print(f\"Processing {key}...\")\n",
    "\n",
    "        # # Dynamically find longitude and latitude coordinates\n",
    "        # lon_name = None\n",
    "        # lat_name = None\n",
    "        # for name in ds.coords:\n",
    "        #     if 'lon' in name.lower() or 'longitude' in name.lower():\n",
    "        #         lon_name = name\n",
    "        #     if 'lat' in name.lower() or 'latitude' in name.lower():\n",
    "        #         lat_name = name\n",
    "\n",
    "        # # Check if lon/lat coordinates were found\n",
    "        # if lon_name is None or lat_name is None:\n",
    "        #     # If not, check for j/i indexing\n",
    "        #     if 'j' in ds.coords and 'i' in ds.coords:\n",
    "        #         print(\"Using j/i indexing instead of lat/lon.\")\n",
    "        #         lon_name = 'i'  # Use 'i' for longitude\n",
    "        #         lat_name = 'j'  # Use 'j' for latitude\n",
    "        #     else:\n",
    "        #         raise ValueError(\"No longitude or latitude coordinates found in the dataset.\")\n",
    "\n",
    "        # # Subset region based on identified coordinates\n",
    "        # if lon_name in ['i', 'j']:  # If using i/j indexing instead of lon/lat\n",
    "        #     subset_ds = ds.isel(\n",
    "        #         **{\n",
    "        #             lon_name: slice(lon_bounds[0], lon_bounds[1]),\n",
    "        #             lat_name: slice(lat_bounds[0], lat_bounds[1]),\n",
    "        #         }\n",
    "        #     )\n",
    "        # else:\n",
    "        #     subset_ds = ds.sel(\n",
    "        #         **{\n",
    "        #             lon_name: slice(lon_bounds[0], lon_bounds[1]),\n",
    "        #             lat_name: slice(lat_bounds[0], lat_bounds[1]),\n",
    "        #         }\n",
    "        #     )\n",
    "        \n",
    "        # # Fix encoding for time coordinate\n",
    "        # # Extract the time encoding and units\n",
    "        # if 'time' in subset_ds.coords:\n",
    "        #     original_time_encoding = ds['time'].encoding\n",
    "        #     units = original_time_encoding.get('units', 'days since 1850-01-01')\n",
    "        #     dtype = original_time_encoding.get('dtype', 'float64')  # Ensure dtype is set\n",
    "\n",
    "        #     # If dtype is None, set it explicitly to 'float64'\n",
    "        #     if dtype is None:\n",
    "        #         dtype = 'float64'\n",
    "\n",
    "        #     # Set time encoding to match units and dtype\n",
    "        #     subset_ds['time'].encoding.update({\n",
    "        #         'units': units,\n",
    "        #         'dtype': dtype,\n",
    "        #         '_FillValue': None,  # Avoid conflicts with fill values\n",
    "        #     })\n",
    "            \n",
    "        #     # If time bounds exist, set units for time_bnds as well\n",
    "        #     if 'time_bnds' in subset_ds.coords:\n",
    "        #         subset_ds['time_bnds'].encoding.update({\n",
    "        #             'units': units,  # Match the units of time\n",
    "        #         })\n",
    "                \n",
    "        #     # Remove chunking to prevent issues with datetime encoding\n",
    "        #     subset_ds['time'].encoding['chunks'] = None\n",
    "                \n",
    "        # # Save the subset file\n",
    "        # subset_ds.to_netcdf(file_name, encoding={var: {} for var in subset_ds.variables})\n",
    "        # print(f\"Saved subset file for {key} to {file_name}\")\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing {key}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New option from pangeo example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using metadta from https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv\n",
      "UKESM1-0-LL\n",
      "{'source_id': 'UKESM1-0-LL', 'ens_members': ['r10i1p1f2']}\n",
      "Trying to download: \n",
      "table_id == 'Omon' & variable_id == 'tos' & experiment_id == 'historical' & source_id == 'UKESM1-0-LL' & member_id == 'r10i1p1f2'\n",
      "Detected curvilinear grid: Subsetting with lat/lon masks.\n",
      "Done subsetting irregular grid\n",
      "Regridding irregular grid\n",
      "<xarray.Dataset>\n",
      "Dimensions:             (time: 1980, lat: 180, lon: 360, vertices: 4, bnds: 2)\n",
      "Coordinates:\n",
      "  * time                (time) object 1850-01-16 00:00:00 ... 2014-12-16 00:0...\n",
      "    time_bnds           (time, bnds) object dask.array<chunksize=(1980, 2), meta=np.ndarray>\n",
      "  * lat                 (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 87.5 88.5 89.5\n",
      "  * lon                 (lon) float64 0.5 1.5 2.5 3.5 ... 357.5 358.5 359.5\n",
      "Dimensions without coordinates: vertices, bnds\n",
      "Data variables:\n",
      "    tos                 (time, lat, lon) float32 dask.array<chunksize=(252, 180, 360), meta=np.ndarray>\n",
      "    vertices_latitude   (vertices, lat, lon) float32 dask.array<chunksize=(4, 180, 360), meta=np.ndarray>\n",
      "    vertices_longitude  (vertices, lat, lon) float32 dask.array<chunksize=(4, 180, 360), meta=np.ndarray>\n",
      "Attributes:\n",
      "    regrid_method:  bilinear\n",
      "saved regridded data:  /Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP5_85/RawTmpFiles/tos_historical_Omon_UKESM1-0-LL_r10i1p1f2_1850_2014_1x1.nc\n",
      "---------------------------------------\n",
      "Trying to download: \n",
      "table_id == 'Omon' & variable_id == 'thetao' & experiment_id == 'historical' & source_id == 'UKESM1-0-LL' & member_id == 'r10i1p1f2'\n",
      "Detected curvilinear grid: Subsetting with lat/lon masks.\n",
      "Done subsetting irregular grid\n",
      "Regridding irregular grid\n",
      "<xarray.Dataset>\n",
      "Dimensions:             (time: 1980, lev: 75, lat: 180, lon: 360, vertices: 4,\n",
      "                         bnds: 2)\n",
      "Coordinates:\n",
      "  * lev                 (lev) float64 0.5058 1.556 2.668 ... 5.698e+03 5.902e+03\n",
      "    lev_bnds            (lev, bnds) float64 dask.array<chunksize=(75, 2), meta=np.ndarray>\n",
      "  * time                (time) object 1850-01-16 00:00:00 ... 2014-12-16 00:0...\n",
      "    time_bnds           (time, bnds) object dask.array<chunksize=(1980, 2), meta=np.ndarray>\n",
      "  * lat                 (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 87.5 88.5 89.5\n",
      "  * lon                 (lon) float64 0.5 1.5 2.5 3.5 ... 357.5 358.5 359.5\n",
      "Dimensions without coordinates: vertices, bnds\n",
      "Data variables:\n",
      "    thetao              (time, lev, lat, lon) float32 dask.array<chunksize=(4, 75, 180, 360), meta=np.ndarray>\n",
      "    vertices_latitude   (vertices, lat, lon) float32 dask.array<chunksize=(4, 180, 360), meta=np.ndarray>\n",
      "    vertices_longitude  (vertices, lat, lon) float32 dask.array<chunksize=(4, 180, 360), meta=np.ndarray>\n",
      "Attributes:\n",
      "    regrid_method:  bilinear\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "# Make sure you are in .conda, then do `conda activate .` \n",
    "# conda install -c conda-forge xesmf making sure you are in .venv/ directory\n",
    "import xesmf as xe\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def is_curvilinear(ds):\n",
    "    lat_name = 'lat' if 'lat' in ds else 'latitude'\n",
    "    lon_name = 'lon' if 'lon' in ds else 'longitude'\n",
    "    \n",
    "    if lat_name in ds and lon_name in ds:\n",
    "        lat_dims = ds[lat_name].dims\n",
    "        lon_dims = ds[lon_name].dims\n",
    "        # Check if lat/lon have 2D dimensions\n",
    "        return len(lat_dims) == 2 and len(lon_dims) == 2\n",
    "    return False\n",
    "\n",
    "def get_lat_lon_names(ds):\n",
    "    \"\"\"\n",
    "    Dynamically detect the latitude and longitude variable names.\n",
    "    \"\"\"\n",
    "    lat_candidates = [\"lat\", \"latitude\"]\n",
    "    lon_candidates = [\"lon\", \"longitude\"]\n",
    "    \n",
    "    lat_name = next((name for name in lat_candidates if name in ds), None)\n",
    "    lon_name = next((name for name in lon_candidates if name in ds), None)\n",
    "    \n",
    "    if lat_name is None or lon_name is None:\n",
    "        raise KeyError(\"Could not find latitude and/or longitude variables in the dataset.\")\n",
    "    \n",
    "    return lat_name, lon_name\n",
    "\n",
    "# Function to plot data\n",
    "def plot_data(ds, variable, title=\"Quick Map\", cmap=\"viridis\", projection=ccrs.PlateCarree()):\n",
    "    if variable not in ds:\n",
    "        raise ValueError(f\"Variable '{variable}' not found in the dataset.\")\n",
    "    # Select a single time step if time is a dimension\n",
    "    if \"time\" in ds.dims:\n",
    "        data = ds[variable].isel(time=0)  # Plot the first time step\n",
    "    else:\n",
    "        data = ds[variable]\n",
    "    # Set up the map\n",
    "    fig, ax = plt.subplots(\n",
    "        subplot_kw={\"projection\": projection},\n",
    "        figsize=(10, 6)\n",
    "    )\n",
    "    ax.set_global()  # Set global extent (you can modify as needed)\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "    ax.add_feature(cfeature.LAND, edgecolor=\"black\", facecolor=\"lightgray\", alpha=0.5)\n",
    "    # Plot the data\n",
    "    im = ax.pcolormesh(\n",
    "        ds[lon_name], ds[lat_name], data,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=cmap\n",
    "    )\n",
    "    # Add colorbar\n",
    "    cb = fig.colorbar(im, ax=ax, orientation=\"horizontal\", pad=0.05)\n",
    "    cb.set_label(variable)\n",
    "    # Title and gridlines\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.gridlines(draw_labels=True, linewidth=0.5, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = \"/Users/aallyn/GitHub/lobSDM/Code/cmip6_input.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    input = json.load(file)\n",
    "# # load the input settings\n",
    "# in_settings = sys.argv[1]\n",
    "# input = json.load(open(in_settings))\n",
    "\n",
    "# load metadata of the data in pangeo\n",
    "if len(input['metadata_csv'].strip()) > 0:\n",
    "    meta_data = pd.read_csv(input['metadata_csv'])\n",
    "    print(\"using local metadta from: \", input['metadata_csv'])\n",
    "else:\n",
    "    print(\"using metadta from https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv\")\n",
    "    meta_data = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "\n",
    "# prepare the output directory\n",
    "out_dir = input['out_dir']\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "data_set=input['dataset']\n",
    "\n",
    "# check if the download all models option is invoked. If True, all unique models and ensemble members are obtained from metadata\n",
    "if input['download_all_models']:\n",
    "    models = meta_data['source_id'].unique()\n",
    "    all_models = True\n",
    "else:\n",
    "    models = data_set.keys()\n",
    "    all_models = False\n",
    "\n",
    "# check if the download all ensemble members option is invoked. If True, all unique models and ensemble members are obtained from metadata\n",
    "if input['download_all_members']:\n",
    "    ens_members = meta_data['member_id'].unique()\n",
    "    full_ensemble = True\n",
    "else:\n",
    "    ens_members = None\n",
    "    full_ensemble = False\n",
    "\n",
    "\n",
    "# check if regrid option is activated and create a lat lon array for target spatial grids\n",
    "if len(input['target_grid']) == 2:\n",
    "    regrid_data = True\n",
    "    tar_lat_int = input['target_grid'][0]\n",
    "    tar_lon_int = input['target_grid'][1]\n",
    "    n_lat = int(180./tar_lat_int)\n",
    "    n_lon = int(360./tar_lon_int)\n",
    "    lat_min = -90 + tar_lat_int/2.\n",
    "    lat_max = 90 - tar_lat_int/2.\n",
    "    lon_max = 360 - tar_lon_int/2.\n",
    "    lon_min = tar_lon_int/2.\n",
    "    new_lat = np.linspace(lat_min, lat_max, n_lat, endpoint=True)\n",
    "    new_lon = np.linspace(lon_min, lon_max, n_lon, endpoint=True)\n",
    "else:\n",
    "    regrid_data = False\n",
    "\n",
    "# loop through the experiments and dataset\n",
    "for experiment, info in input['experiments'].items():\n",
    "    start_year = info[0]\n",
    "    end_year = info[1]\n",
    "    # loop through the unique models\n",
    "    for model in models:\n",
    "        # get models if download_all_models/all_models is False\n",
    "        if not all_models:\n",
    "            print(model)\n",
    "            print(data_set[model])\n",
    "            src = data_set[model]['source_id']\n",
    "        else:\n",
    "            src = model\n",
    "\n",
    "        # get ensemble members if download_all_members/full_ensemble is False\n",
    "        if not full_ensemble:\n",
    "            ens_members = data_set[model]['ens_members']\n",
    "        # loop through the variables\n",
    "        for variable, table in input['variables'].items():\n",
    "            # loop through the ensemble\n",
    "            for variant in ens_members:\n",
    "                qry = \"table_id == '\" + table +\"' & variable_id == '\" + variable + \"' & experiment_id == '\" + experiment + \"' & source_id == '\" + src + \"' & member_id == '\" + variant + \"'\"\n",
    "                meta_data_sel = meta_data.query(qry)\n",
    "                \n",
    "                # Detect latitude and longitude variable names\n",
    "                try:\n",
    "                    lat_name, lon_name = get_lat_lon_names(ds)\n",
    "                except KeyError as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                # Continue\n",
    "                if not meta_data_sel.empty:\n",
    "                    print(\"Trying to download: \")\n",
    "                    print(qry)\n",
    "                    # print(meta_data_sel)\n",
    "                    zstore = meta_data_sel.zstore.values[-1]\n",
    "                    # create a mutable-mapping-style interface to the store\n",
    "                    mapper = fsspec.get_mapper(zstore)\n",
    "                    # open it using xarray and zarr\n",
    "                    ds = xr.open_zarr(mapper, consolidated=True)\n",
    "                    \n",
    "                    curvilinear_flag = is_curvilinear(ds)\n",
    "                    if curvilinear_flag:\n",
    "                        print(\"Detected curvilinear grid: Subsetting with lat/lon masks.\")\n",
    "                        # Subset using lat/lon bounds with 2D latitude/longitude variables\n",
    "                        # lat_2d = ds[lat_name]\n",
    "                        # lon_2d = ds[lon_name]\n",
    "                        \n",
    "                        # lat_mask = (lat_2d >= lat_bounds[0]) & (lat_2d <= lat_bounds[1])\n",
    "                        # lon_mask = (lon_2d >= lon_bounds[0]) & (lon_2d <= lon_bounds[1])\n",
    "                        # region_mask = lat_mask & lon_mask\n",
    "                        \n",
    "                        # # Compute the mask to resolve the Dask-backed operation\n",
    "                        # region_mask = region_mask.compute()\n",
    "\n",
    "                        # ds_subset = ds.where(region_mask, drop=True)\n",
    "                        \n",
    "                        # lat_min_idx = (ds[lat_name] >= lat_bounds[0]).argmax(dim='j').values  # Find the index where lat >= lat_min\n",
    "                        # lat_max_idx = (ds[lat_name] <= lat_bounds[1]).argmin(dim='j').values  # Find the index where lat <= lat_max\n",
    "                        # print(ds['longitude'])\n",
    "                        # lon_min_idx = (ds[lon_name] >= lon_bounds[0]).argmax(dim='i').values   # Find the index where lon >= lon_min\n",
    "                        # lon_max_idx = (ds[lon_name] <= lon_bounds[1]).argmin(dim='i').values  # Find the index where lon <= lon_max\n",
    "                        # # print(lon_max_idx)\n",
    "                        \n",
    "                        ds_subset = ds.isel(\n",
    "                            j=slice(220, 275),\n",
    "                            i=slice(200, 235)\n",
    "                        )\n",
    "                        \n",
    "                        ds_out_temp = ds_subset.sel(time=slice(str(start_year), str(end_year)))\n",
    "                        print(\"Done subsetting irregular grid\")\n",
    "                        # plot_data(ds_out_temp, variable, title=\"Quick Map\", cmap=\"viridis\", projection=ccrs.PlateCarree())\n",
    "\n",
    "                        if regrid_data:\n",
    "                            print(\"Regridding irregular grid\")\n",
    "                            # create dataset with target grid\n",
    "                            ds_out_target = xr.Dataset({'lat': (['lat'], new_lat), 'lon': (['lon'], new_lon),})\n",
    "                            regridder = xe.Regridder(ds_out_temp, ds_out_target, input['regrid_method'])\n",
    "                            ds_out = regridder(ds_out_temp)\n",
    "                            print(ds_out)\n",
    "\n",
    "                    else:\n",
    "                        print(\"Detected regular grid or indices: Subsetting with lat/lon or latitude/longitude.\")\n",
    "                        # Identify the correct coordinate names\n",
    "                        lon_name = 'lon' if 'lon' in ds.coords and 'lon' in ds.variables else 'longitude'\n",
    "                        lat_name = 'lat' if 'lat' in ds.coords and 'lat' in ds.variables else 'latitude'\n",
    "                                                \n",
    "                        # Perform subsetting\n",
    "                        ds_subset = ds.sel(\n",
    "                            **{\n",
    "                                lon_name: slice(lon_bounds[0], lon_bounds[1]),\n",
    "                                lat_name: slice(lat_bounds[0], lat_bounds[1]),\n",
    "                            }\n",
    "                        )\n",
    "                        ds_out_temp = ds_subset.sel(time=slice(str(start_year), str(end_year)))\n",
    "                        print(\"Done subsetting regular grid\")\n",
    "                        \n",
    "                        if regrid_data:\n",
    "                            # regrid if the option is invoked\n",
    "                            print(\"Regridding regular grid\")\n",
    "                            # create dataset with target grid\n",
    "                            ds_out_target = xr.Dataset({'lat': (['lat'], new_lat), 'lon': (['lon'], new_lon),})\n",
    "                            # Regrid\n",
    "                            regridder = xe.Regridder(ds_out_temp, ds_out_target, input['regrid_method'])\n",
    "                            ds_out = regridder(ds_out_temp)\n",
    "                        \n",
    "                    # create and save the output\n",
    "                    file_name_nc_out = '{var}_{exp}_{tab}_{sroc}_{vrnt}_{syr}_{eyr}_{t_lat}x{t_lon}.nc'.format(var=variable, exp=experiment, tab=table, sroc=src, vrnt=variant, syr=str(start_year), eyr=end_year, t_lat=str(tar_lat_int), t_lon=str(tar_lon_int))\n",
    "                    out_file_out = os.path.join(out_dir, file_name_nc_out)\n",
    "                    ds_out.to_netcdf(out_file_out)\n",
    "                    \n",
    "                    # close dataset\n",
    "                    ds_out.close()\n",
    "                    ds_out_temp.close()\n",
    "                    print('saved regridded data: ', out_file_out)\n",
    "                    \n",
    "                    # close dataset\n",
    "                    ds.close()\n",
    "                else:\n",
    "                    print(f'Data not found for: {variable}')\n",
    "                    print(qry)\n",
    "                print ('---------------------------------------')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
